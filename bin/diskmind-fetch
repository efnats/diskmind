#!/usr/bin/env python3
"""
diskmind-fetch - Data Collection
Collects SMART data from remote hosts via SSH and stores in SQLite.

Can also be imported as a library for use by diskmind-view's /api/ingest endpoint.
"""
import sys
sys.dont_write_bytecode = True

import argparse
import csv
import io
import os
import sqlite3
import subprocess
import sys
from datetime import datetime, timezone
from pathlib import Path

# Shared library
sys.path.insert(0, str(Path(__file__).resolve().parent.parent / 'lib'))
from diskmind_core import VERSION, parse_simple_yaml, load_thresholds_from_dir, generate_alerts, send_notifications


# ---------------------------------------------------------------------------
# Library Functions (importable by diskmind-view)
# ---------------------------------------------------------------------------

def parse_csv(csv_data: str, host: str = None) -> list[dict]:
    """Parse CSV data from diskmind-scan output.
    
    Args:
        csv_data: CSV string with header row
        host: Optional host to set on all readings (overrides any host in data)
    
    Returns:
        List of reading dicts
    """
    readings = []
    reader = csv.DictReader(io.StringIO(csv_data))
    for row in reader:
        if host:
            row['host'] = host
        readings.append(row)
    return readings


def store_readings(conn: sqlite3.Connection, readings: list[dict], timestamp: str, source: str = None):
    """Store readings in database."""
    cursor = conn.cursor()
    
    for r in readings:
        try:
            wwn = r.get('wwn', '').strip()
            serial = r.get('serial', '').strip()
            # Primary identifier: WWN if available, otherwise serial
            disk_id = wwn if wwn else serial
            
            # Deduplicate: skip if reading for this disk exists within last 2 minutes
            cursor.execute('''
                SELECT 1 FROM readings
                WHERE disk_id = ? AND timestamp > datetime(?, '-2 minutes')
                LIMIT 1
            ''', (disk_id, timestamp))
            if cursor.fetchone():
                continue
            
            # Record first time this disk was seen (no-op if already exists)
            cursor.execute('''
                INSERT OR IGNORE INTO disk_first_seen (disk_id, first_seen)
                VALUES (?, ?)
            ''', (disk_id, timestamp))
            
            cursor.execute('''
                INSERT OR REPLACE INTO readings 
                (disk_id, wwn, serial, timestamp, host, device, type, model,
                 capacity_bytes, firmware, rpm, sector_size,
                 smart_status, smart_attributes, source)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                disk_id,
                wwn or None,
                serial,
                timestamp,
                r.get('host'),
                r.get('device'),
                r.get('type'),
                r.get('model'),
                int(r.get('capacity_bytes') or 0),
                r.get('firmware') or None,
                int(r.get('rpm') or 0) or None,
                int(r.get('sector_size') or 0) or None,
                r.get('smart_status'),
                r.get('smart_attributes', '{}'),
                source,
            ))
        except Exception as e:
            print(f"  Warning: Failed to store {r.get('serial')}: {e}", file=sys.stderr)
    
    conn.commit()


def update_host_status(conn: sqlite3.Connection, host: str, status: str, message: str, disk_count: int, timestamp: str):
    """Update host status in database."""
    cursor = conn.cursor()
    
    if status == 'ok':
        cursor.execute('''
            INSERT OR REPLACE INTO host_status (host, status, message, disk_count, last_attempt, last_success)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (host, status, message, disk_count, timestamp, timestamp))
    else:
        # On failure, preserve last_success if it exists
        cursor.execute('''
            INSERT INTO host_status (host, status, message, disk_count, last_attempt, last_success)
            VALUES (?, ?, ?, ?, ?, NULL)
            ON CONFLICT(host) DO UPDATE SET
                status = excluded.status,
                message = excluded.message,
                disk_count = excluded.disk_count,
                last_attempt = excluded.last_attempt
        ''', (host, status, message, disk_count, timestamp))
    
    conn.commit()


def init_database(db_path: str) -> sqlite3.Connection:
    """Initialize SQLite database with WAL mode for better concurrency."""
    Path(db_path).parent.mkdir(parents=True, exist_ok=True)
    conn = sqlite3.connect(db_path)

    # WAL mode allows concurrent reads while writing
    conn.execute("PRAGMA journal_mode=WAL")

    conn.executescript('''
        CREATE TABLE IF NOT EXISTS readings (
            disk_id TEXT NOT NULL,
            wwn TEXT,
            serial TEXT NOT NULL,
            timestamp DATETIME NOT NULL,
            host TEXT NOT NULL,
            device TEXT NOT NULL,
            type TEXT,
            model TEXT,
            capacity_bytes INTEGER,
            firmware TEXT,
            rpm INTEGER,
            sector_size INTEGER,
            smart_status TEXT,
            smart_attributes TEXT,
            source TEXT,
            PRIMARY KEY (disk_id, timestamp)
        );
        CREATE INDEX IF NOT EXISTS idx_readings_timestamp ON readings(timestamp);
        CREATE INDEX IF NOT EXISTS idx_readings_host ON readings(host);
        CREATE INDEX IF NOT EXISTS idx_readings_wwn ON readings(wwn);
        
        CREATE TABLE IF NOT EXISTS host_status (
            host TEXT PRIMARY KEY,
            status TEXT NOT NULL,
            message TEXT,
            disk_count INTEGER DEFAULT 0,
            last_attempt DATETIME,
            last_success DATETIME
        );
        
        CREATE TABLE IF NOT EXISTS push_attempts (
            host TEXT PRIMARY KEY,
            last_attempt DATETIME,
            attempts INTEGER DEFAULT 1,
            reason TEXT DEFAULT 'unknown'
        );
        
        CREATE TABLE IF NOT EXISTS disk_first_seen (
            disk_id TEXT PRIMARY KEY,
            first_seen DATETIME NOT NULL
        );
        
        CREATE TABLE IF NOT EXISTS disk_status (
            disk_id TEXT PRIMARY KEY,
            smart_status TEXT,
            smart_attributes TEXT,
            status TEXT NOT NULL DEFAULT 'ok',
            updated_at DATETIME
        );
        
        CREATE TABLE IF NOT EXISTS alerts (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            disk_id TEXT NOT NULL,
            host TEXT NOT NULL,
            timestamp DATETIME NOT NULL,
            alert_type TEXT NOT NULL,
            severity TEXT NOT NULL,
            attribute TEXT,
            old_value TEXT,
            new_value TEXT,
            message TEXT,
            acknowledged INTEGER DEFAULT 0
        );
        CREATE INDEX IF NOT EXISTS idx_alerts_timestamp ON alerts(timestamp);
        CREATE INDEX IF NOT EXISTS idx_alerts_severity ON alerts(severity);
        
        CREATE TABLE IF NOT EXISTS notification_log (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            alert_id INTEGER,
            timestamp DATETIME NOT NULL,
            channel TEXT NOT NULL,
            success INTEGER DEFAULT 0,
            error TEXT
        );
    ''')
    
    return conn


# ---------------------------------------------------------------------------
# SSH Collection (CLI only)
# ---------------------------------------------------------------------------

def load_config(config_path: str) -> dict:
    """Load configuration from YAML file."""
    with open(config_path) as f:
        config = parse_simple_yaml(f.read())
    
    # Load hosts from file if specified
    if 'hosts_file' in config:
        with open(config['hosts_file']) as f:
            config['hosts'] = [
                line.strip() for line in f 
                if line.strip() and not line.startswith('#')
            ]
    
    return config


def get_local_script_path() -> str:
    """Get path to diskmind-scan."""
    base_dir = Path(__file__).parent.parent
    candidates = [
        Path(__file__).parent / 'diskmind-scan',   # bin/diskmind-scan (same dir)
        base_dir / 'bin' / 'diskmind-scan',         # explicit bin/
        base_dir / 'diskmind-scan',                  # project root
    ]
    
    for path in candidates:
        if path.exists():
            return str(path)
    
    # Fallback: expected location
    return str(candidates[0])


def collect_from_host(host: str, ssh_user: str, ssh_timeout: int, ssh_port: int = None) -> dict:
    """Collect SMART data from a single host via SSH (or locally for localhost).
    
    Returns dict with:
        - status: 'ok', 'offline', 'auth_failed', 'timeout', 'no_smartctl', 'no_disks', 'error'
        - message: Human-readable error message (if any)
        - readings: List of disk readings (if successful)
    """
    script_path = get_local_script_path()
    
    # Read script content
    with open(script_path) as f:
        script_content = f.read()
    
    try:
        # Execute locally for localhost
        if host in ('localhost', '127.0.0.1'):
            result = subprocess.run(
                ['bash', '-c', script_content],
                capture_output=True,
                text=True,
                timeout=120
            )
        else:
            # Build SSH command
            ssh_cmd = [
                'ssh',
                '-o', 'BatchMode=yes',
                '-o', f'ConnectTimeout={ssh_timeout}',
                '-o', 'StrictHostKeyChecking=accept-new',
            ]
            # Add port if specified
            if ssh_port:
                ssh_cmd.extend(['-p', str(ssh_port)])
            ssh_cmd.extend([
                f'{ssh_user}@{host}',
                'bash -s'
            ])
            
            # Execute script on remote host via SSH
            result = subprocess.run(
                ssh_cmd,
                input=script_content,
                capture_output=True,
                text=True,
                timeout=ssh_timeout + 60
            )
        
        stderr_lower = result.stderr.lower()
        
        if result.returncode != 0:
            # Classify the error
            if 'permission denied' in stderr_lower or 'authentication failed' in stderr_lower:
                return {'status': 'auth_failed', 'message': 'Permission denied', 'readings': []}
            elif 'connection refused' in stderr_lower:
                return {'status': 'offline', 'message': 'Connection refused', 'readings': []}
            elif 'no route to host' in stderr_lower:
                return {'status': 'offline', 'message': 'No route to host', 'readings': []}
            elif 'host is down' in stderr_lower or 'host unreachable' in stderr_lower:
                return {'status': 'offline', 'message': 'Host unreachable', 'readings': []}
            elif 'name or service not known' in stderr_lower or 'could not resolve' in stderr_lower:
                return {'status': 'offline', 'message': 'DNS resolution failed', 'readings': []}
            elif 'smartctl' in stderr_lower and 'not found' in stderr_lower:
                return {'status': 'no_smartctl', 'message': 'smartctl not installed', 'readings': []}
            else:
                error_msg = result.stderr.strip() or result.stdout.strip() or f"Exit code {result.returncode}"
                return {'status': 'error', 'message': error_msg[:100], 'readings': []}
        
        # Check if output is empty
        if not result.stdout.strip():
            return {'status': 'no_smartctl', 'message': 'No output (smartctl missing or no root)', 'readings': []}
        
        # Parse CSV using library function
        readings = parse_csv(result.stdout, host=host)
        
        if not readings:
            return {'status': 'no_disks', 'message': 'No disks found', 'readings': []}
        
        return {'status': 'ok', 'message': None, 'readings': readings}
        
    except subprocess.TimeoutExpired:
        return {'status': 'timeout', 'message': f'Timeout after {ssh_timeout}s', 'readings': []}
    except FileNotFoundError as e:
        return {'status': 'error', 'message': f'{e.filename} not found', 'readings': []}
    except Exception as e:
        return {'status': 'error', 'message': str(e)[:100], 'readings': []}


def cleanup_old_data(conn: sqlite3.Connection, retention_days: int):
    """Remove data older than retention period."""
    cursor = conn.cursor()
    cursor.execute('''
        DELETE FROM readings 
        WHERE timestamp < datetime('now', ?)
    ''', (f'-{retention_days} days',))
    
    deleted = cursor.rowcount
    if deleted > 0:
        print(f"  Cleaned up {deleted} old records")
    
    conn.commit()


def main():
    parser = argparse.ArgumentParser(
        description='Collect SMART data from remote hosts'
    )
    parser.add_argument(
        '-c', '--config',
        default='config/config.yaml',
        help='Path to config file (default: config/config.yaml)'
    )
    parser.add_argument(
        '--hosts',
        help='Comma-separated list of hosts (overrides config)'
    )
    parser.add_argument(
        '--db',
        help='Database path (overrides config)'
    )
    parser.add_argument(
        '-v', '--verbose',
        action='store_true',
        help='Verbose output'
    )
    
    args = parser.parse_args()
    
    # Load config
    if os.path.exists(args.config):
        config = load_config(args.config)
    else:
        config = {'hosts': [], 'ssh': {}, 'database': {}}
    
    # Override with CLI args
    if args.hosts:
        config['hosts'] = [h.strip() for h in args.hosts.split(',')]
    if args.db:
        config['database']['path'] = args.db
    
    # Validate
    if not config.get('hosts'):
        # Default to localhost if no hosts specified
        config['hosts'] = ['localhost']
        print("No hosts specified, using localhost")
    
    # Settings
    db_path = config.get('database', {}).get('path', './data/diskmind.db')
    ssh_timeout = config.get('ssh', {}).get('timeout', 30)
    retention_days = config.get('database', {}).get('retention_days', 365)
    
    # Parse hosts - support "method:user@host" or "method:user@host:port" format
    # push: hosts are skipped (they push data themselves)
    # ssh: or no prefix = collect via SSH
    # User must always be specified: user@host or user@host:port
    raw_hosts = config.get('hosts', [])
    hosts = []
    for h in raw_hosts:
        h = str(h).strip()
        # Skip push hosts
        if h.startswith('push:'):
            continue
        # Strip ssh: prefix if present
        if h.startswith('ssh:'):
            h = h[4:]
        if '@' in h:
            user, rest = h.split('@', 1)
            # Check for port (format: host:port)
            if ':' in rest:
                ip, port = rest.rsplit(':', 1)
                # Validate port is numeric
                if port.isdigit():
                    hosts.append({'ip': ip, 'user': user, 'port': int(port)})
                else:
                    hosts.append({'ip': rest, 'user': user, 'port': None})
            else:
                hosts.append({'ip': rest, 'user': user, 'port': None})
        else:
            print(f"  Warning: Skipping '{h}' — missing user. Use user@host format (e.g. root@{h})", file=sys.stderr)
    
    # Initialize
    print(f"diskmind-fetch {VERSION}")
    print("=" * 40)
    print(f"Hosts: {len(hosts)}")
    print(f"Database: {db_path}")
    print()
    
    conn = init_database(db_path)
    timestamp = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')
    
    # Collect from all hosts
    total_disks = 0
    successful_hosts = 0
    all_readings = []
    
    print("Collecting data:")
    for host_info in hosts:
        host = host_info['ip']
        ssh_user = host_info['user']
        ssh_port = host_info.get('port')
        port_str = f":{ssh_port}" if ssh_port else ""
        print(f"  → {ssh_user}@{host}{port_str}...", end=' ', flush=True)
        
        result = collect_from_host(host, ssh_user, ssh_timeout, ssh_port)
        readings = result['readings']
        
        # Update host status in DB
        update_host_status(conn, host, result['status'], result['message'], len(readings), timestamp)
        
        if result['status'] == 'ok':
            store_readings(conn, readings, timestamp, source='ssh')
            all_readings.extend(readings)
            total_disks += len(readings)
            successful_hosts += 1
            print(f"✓ {len(readings)} disks")
        else:
            msg = result['message'] or result['status']
            print(f"✗ {msg}")
    
    # Generate alerts and send notifications
    if all_readings:
        config_dir = Path(__file__).resolve().parent.parent / 'config'
        thresholds = load_thresholds_from_dir(config_dir)
        new_alerts = generate_alerts(conn, all_readings, thresholds, timestamp)
        if new_alerts:
            notify_config = config.get('notifications', {})
            send_notifications(conn, new_alerts, notify_config,
                               config.get('webhook_urls', []))
            print(f"  Alerts: {len(new_alerts)} generated")
    
    print()
    print(f"Collected: {total_disks} disks from {successful_hosts}/{len(hosts)} hosts")
    
    # Cleanup old data
    if retention_days > 0:
        cleanup_old_data(conn, retention_days)
    
    conn.close()
    
    print("Done!")
    return 0 if successful_hosts > 0 else 1


if __name__ == '__main__':
    sys.exit(main())
