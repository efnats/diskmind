#!/usr/bin/env python3
"""
diskmind - Web Server
Serves live reports from SQLite database.
"""
import sys
sys.dont_write_bytecode = True

import argparse
import importlib.machinery
import importlib.util
import json
import os
import sqlite3
import subprocess
import sys
import threading
import time
from datetime import datetime, timezone
from http.server import HTTPServer, BaseHTTPRequestHandler, ThreadingHTTPServer
from pathlib import Path
from urllib.parse import urlparse, parse_qs

# Optional: yaml for config
try:
    import yaml
    HAS_YAML = True
except ImportError:
    HAS_YAML = False

from collections import defaultdict

# ---------------------------------------------------------------------------
# Rate Limiting
# ---------------------------------------------------------------------------

_rate_limit_requests = defaultdict(list)

def check_rate_limit(ip: str) -> bool:
    """Return True if request is allowed, False if rate limited."""
    config = load_config()
    rate_config = config.get('rate_limit', {})
    max_requests = rate_config.get('max_requests', 10)
    window_seconds = rate_config.get('window_seconds', 60)
    
    # Disabled if max_requests is 0
    if max_requests <= 0:
        return True
    
    now = time.time()
    # Remove timestamps outside the window
    _rate_limit_requests[ip] = [t for t in _rate_limit_requests[ip] if now - t < window_seconds]
    # Check limit
    if len(_rate_limit_requests[ip]) >= max_requests:
        return False
    _rate_limit_requests[ip].append(now)
    return True

# ---------------------------------------------------------------------------
# Database Functions
# ---------------------------------------------------------------------------

def get_db_connection(db_path: str) -> sqlite3.Connection:
    """Get database connection with row factory."""
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    return conn


# Track last cleanup time
_last_cleanup = 0


def cleanup_old_readings(db_path: str):
    """Remove readings older than retention_days. Runs at most once per hour."""
    global _last_cleanup
    now = time.time()
    if now - _last_cleanup < 3600:
        return
    _last_cleanup = now
    
    config = load_config()
    retention_days = config.get('database', {}).get('retention_days', 365)
    if retention_days <= 0:
        return
    
    conn = get_db_connection(db_path)
    fm = get_fetch_module()
    fm.cleanup_old_data(conn, retention_days)
    conn.close()


# Delta preset mappings (preset name -> days)
DELTA_PRESETS = {
    '1h': 1/24,
    '24h': 1,
    '7d': 7,
    '30d': 30,
    '90d': 90,
    'all': 36500,
}


def get_current_readings(db_path: str) -> list[dict]:
    """Get most recent reading for each disk."""
    conn = get_db_connection(db_path)
    cursor = conn.cursor()
    
    # Get latest reading per disk_id
    cursor.execute('''
        SELECT * FROM readings
        WHERE (disk_id, timestamp) IN (
            SELECT disk_id, MAX(timestamp) FROM readings GROUP BY disk_id
        )
        ORDER BY host, device
    ''')
    
    results = [dict(row) for row in cursor.fetchall()]
    conn.close()
    return results


def get_trends(db_path: str, days: int = 30) -> dict:
    """Get sector trends for each disk (reallocated sectors change over time)."""
    conn = get_db_connection(db_path)
    cursor = conn.cursor()
    
    # Get all readings from last N days
    cursor.execute('''
        SELECT disk_id, smart_attributes, timestamp
        FROM readings
        WHERE timestamp > datetime('now', ?)
        ORDER BY disk_id, timestamp
    ''', (f'-{days} days',))
    
    # Group by disk_id and extract reallocated sectors
    by_disk = {}
    for row in cursor.fetchall():
        disk_id = row['disk_id']
        if disk_id not in by_disk:
            by_disk[disk_id] = []
        
        attrs = row['smart_attributes']
        if isinstance(attrs, str):
            try:
                attrs = json.loads(attrs)
            except:
                attrs = {}
        
        realloc = int(attrs.get('Reallocated_Sector_Ct', 0) or 0)
        by_disk[disk_id].append(realloc)
    
    # Calculate trends
    trends = {}
    for disk_id, values in by_disk.items():
        if len(values) >= 2:
            delta = values[-1] - values[0]
            if delta != 0:
                trends[disk_id] = {
                    'old': values[0],
                    'new': values[-1],
                    'delta': delta
                }
    
    conn.close()
    return trends


def get_disk_history(db_path: str, disk_id: str = None, days: int = 30) -> dict:
    """Get historical attribute values for sparkline rendering.
    Returns {disk_id: {attr_name: [{timestamp, value}, ...], ...}, ...}
    """
    conn = get_db_connection(db_path)
    cursor = conn.cursor()

    if disk_id:
        cursor.execute('''
            SELECT disk_id, timestamp, smart_attributes, type
            FROM readings
            WHERE disk_id = ? AND timestamp > datetime('now', ?)
            ORDER BY timestamp
        ''', (disk_id, f'-{days} days'))
    else:
        cursor.execute('''
            SELECT disk_id, timestamp, smart_attributes, type
            FROM readings
            WHERE timestamp > datetime('now', ?)
            ORDER BY disk_id, timestamp
        ''', (f'-{days} days',))

    history = {}
    for row in cursor.fetchall():
        did = row['disk_id']
        if did not in history:
            history[did] = {'_type': row['type'], '_timestamps': []}

        ts = row['timestamp']
        history[did]['_timestamps'].append(ts)

        attrs = row['smart_attributes']
        if isinstance(attrs, str):
            try:
                attrs = json.loads(attrs)
            except:
                attrs = {}

        for attr_name, value in attrs.items():
            if attr_name not in history[did]:
                history[did][attr_name] = []
            try:
                history[did][attr_name].append({'t': ts, 'v': float(value)})
            except (ValueError, TypeError):
                pass

    # Load true first-seen timestamps (not affected by query window or retention)
    first_seen = {}
    try:
        cursor.execute('SELECT disk_id, first_seen FROM disk_first_seen')
        for row in cursor.fetchall():
            first_seen[row['disk_id']] = row['first_seen']
    except Exception:
        pass  # Table may not exist yet (old schema)

    conn.close()

    # Compute per-attribute summary: current, delta, points array
    result = {}
    for did, attr_data in history.items():
        disk_type = attr_data.pop('_type', None)
        timestamps = attr_data.pop('_timestamps', [])
        result[did] = {
            '_type': disk_type,
            '_readings': len(set(timestamps)),
            '_first': first_seen.get(did) or (timestamps[0] if timestamps else None),
            '_last': timestamps[-1] if timestamps else None,
        }
        for attr_name, points in attr_data.items():
            if len(points) < 1:
                continue
            values = [p['v'] for p in points]
            result[did][attr_name] = {
                'points': values,
                'current': values[-1],
                'delta': values[-1] - values[0] if len(values) >= 2 else 0,
                'min': min(values),
                'max': max(values),
            }

    return result


def get_hosts(db_path: str) -> list[str]:
    """Get list of all hosts."""
    conn = get_db_connection(db_path)
    cursor = conn.cursor()
    cursor.execute('SELECT DISTINCT host FROM readings ORDER BY host')
    hosts = [row['host'] for row in cursor.fetchall()]
    conn.close()
    return hosts


def get_host_status(db_path: str) -> dict:
    """Get host status from host_status table."""
    conn = get_db_connection(db_path)
    cursor = conn.cursor()
    
    # Check if table exists
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='host_status'")
    if not cursor.fetchone():
        conn.close()
        return {}
    
    cursor.execute('''
        SELECT host, status, message, disk_count, last_attempt, last_success
        FROM host_status
    ''')
    result = {}
    for row in cursor.fetchall():
        result[row['host']] = {
            'status': row['status'],
            'message': row['message'],
            'disk_count': row['disk_count'],
            'last_attempt': row['last_attempt'],
            'last_success': row['last_success'],
        }
    conn.close()
    return result


def get_push_attempts(db_path: str) -> dict:
    """Get rejected push attempts (hosts trying to push while configured as SSH or unknown)."""
    conn = get_db_connection(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='push_attempts'")
    if not cursor.fetchone():
        conn.close()
        return {}
    # Check if reason column exists
    cursor.execute("PRAGMA table_info(push_attempts)")
    cols = [row[1] for row in cursor.fetchall()]
    if 'reason' in cols:
        cursor.execute('SELECT host, last_attempt, attempts, reason FROM push_attempts')
    else:
        cursor.execute('SELECT host, last_attempt, attempts FROM push_attempts')
    result = {}
    for row in cursor.fetchall():
        result[row['host']] = {
            'last_attempt': row['last_attempt'],
            'attempts': row['attempts'],
            'reason': row['reason'] if 'reason' in cols else 'unknown',
        }
    conn.close()
    return result


def get_host_stats(db_path: str) -> dict:
    """Get per-host last scan time and disk count."""
    conn = get_db_connection(db_path)
    cursor = conn.cursor()
    cursor.execute('''
        SELECT host,
               MAX(timestamp) as last_seen,
               COUNT(DISTINCT serial) as disk_count
        FROM readings
        GROUP BY host
    ''')
    stats = {}
    for row in cursor.fetchall():
        stats[row['host']] = {
            'last_seen': row['last_seen'],
            'disk_count': row['disk_count'],
        }
    conn.close()
    return stats


def get_stats(readings: list[dict]) -> dict:
    """Calculate summary statistics."""
    total = len(readings)
    critical = 0
    warning = 0
    healthy = 0
    total_capacity = 0
    
    for r in readings:
        total_capacity += r.get('capacity_bytes') or 0
        
        status = classify_disk(r)
        if status == 'critical':
            critical += 1
        elif status == 'warning':
            warning += 1
        else:
            healthy += 1
    
    return {
        'total': total,
        'critical': critical,
        'warning': warning,
        'healthy': healthy,
        'total_capacity_tb': round(total_capacity / 1e12, 1) if total_capacity else 0,
    }


# ---------------------------------------------------------------------------
# Threshold-based Health Classification
# ---------------------------------------------------------------------------

DEFAULT_PRESET = 'backblaze'

def _find_thresholds_file() -> Path | None:
    """Locate config/thresholds.json."""
    base_dir = Path(__file__).parent.parent
    candidates = [
        base_dir / 'config' / 'thresholds.json',
        base_dir / 'lib' / 'thresholds.json',
        base_dir / 'thresholds.json',
        Path(__file__).parent / 'thresholds.json',
    ]
    for path in candidates:
        if path.exists():
            return path
    return None


def _parse_simple_yaml(text: str) -> dict:
    """Parse simple YAML (flat keys, string values, simple lists). No PyYAML needed."""
    result = {}
    current_key = None
    current_list = None
    for raw_line in text.split('\n'):
        stripped = raw_line.strip()
        if not stripped or stripped.startswith('#'):
            continue
        indent = len(raw_line) - len(raw_line.lstrip())
        if indent > 0 and current_key and stripped.startswith('- '):
            val = stripped[2:].strip()
            if current_list is None:
                current_list = []
                result[current_key] = current_list
            current_list.append(val)
        elif indent > 0 and current_key and ':' in stripped:
            k, _, v = stripped.partition(':')
            v = v.strip()
            if not isinstance(result.get(current_key), dict):
                result[current_key] = {}
            if v:
                try:
                    result[current_key][k.strip()] = int(v)
                except ValueError:
                    result[current_key][k.strip()] = v
        elif ':' in stripped and indent == 0:
            k, _, v = stripped.partition(':')
            k = k.strip()
            v = v.strip()
            current_key = k
            current_list = None
            if v:
                try:
                    result[k] = int(v)
                except ValueError:
                    result[k] = v
    return result


def _get_config_path() -> Path:
    """Get path to config/config.yaml."""
    base_dir = Path(__file__).parent.parent
    return base_dir / 'config' / 'config.yaml'


def load_config() -> dict:
    """Load user config from config/config.yaml."""
    config_path = _get_config_path()
    if config_path.exists():
        try:
            with open(config_path) as f:
                return _parse_simple_yaml(f.read())
        except IOError:
            pass
    return {}


def save_config_value(key: str, value):
    """Update a single top-level key in config/config.yaml, preserving comments.
    If value is None, removes the key from the config file."""
    config_path = _get_config_path()
    config_path.parent.mkdir(parents=True, exist_ok=True)
    
    if config_path.exists():
        lines = config_path.read_text().splitlines()
    else:
        lines = []
    
    # Find and replace existing key, or append
    found = False
    for i, line in enumerate(lines):
        stripped = line.strip()
        if stripped.startswith('#'):
            continue
        if ':' in stripped:
            k, _, _ = stripped.partition(':')
            if k.strip() == key:
                if value is None:
                    lines.pop(i)
                else:
                    lines[i] = f'{key}: {value}'
                found = True
                break
    if not found and value is not None:
        lines.append(f'{key}: {value}')
    
    config_path.write_text('\n'.join(lines) + '\n')


def save_config_list(key: str, items: list):
    """Update a top-level list key in config/config.yaml, preserving comments."""
    config_path = _get_config_path()
    config_path.parent.mkdir(parents=True, exist_ok=True)
    
    if config_path.exists():
        lines = config_path.read_text().splitlines()
    else:
        lines = []
    
    # Find key, remove old list items, insert new ones
    key_idx = None
    remove_end = None
    for i, line in enumerate(lines):
        stripped = line.strip()
        if stripped.startswith('#'):
            continue
        if key_idx is not None:
            # We're inside the list — consume indented lines
            indent = len(line) - len(line.lstrip())
            if indent > 0 and (stripped.startswith('- ') or stripped.startswith('#')):
                remove_end = i + 1
            else:
                break
        elif ':' in stripped:
            k, _, _ = stripped.partition(':')
            if k.strip() == key:
                key_idx = i
                remove_end = i + 1
    
    new_lines = [f'  - {item}' for item in items]
    
    if key_idx is not None:
        lines[key_idx:remove_end] = [f'{key}:'] + new_lines
    else:
        lines.append(f'{key}:')
        lines.extend(new_lines)
    
    config_path.write_text('\n'.join(lines) + '\n')


def save_config_subkey(section: str, key: str, value):
    """Update a nested key (e.g. ssh.user) in config/config.yaml."""
    config_path = _get_config_path()
    config_path.parent.mkdir(parents=True, exist_ok=True)
    
    if config_path.exists():
        lines = config_path.read_text().splitlines()
    else:
        lines = []
    
    section_idx = None
    for i, line in enumerate(lines):
        stripped = line.strip()
        if stripped.startswith('#'):
            continue
        if section_idx is not None:
            indent = len(line) - len(line.lstrip())
            if indent > 0 and ':' in stripped:
                k, _, _ = stripped.partition(':')
                if k.strip() == key:
                    lines[i] = f'  {key}: {value}'
                    config_path.write_text('\n'.join(lines) + '\n')
                    return
            elif indent == 0 and stripped:
                # Past the section, insert before this line
                lines.insert(i, f'  {key}: {value}')
                config_path.write_text('\n'.join(lines) + '\n')
                return
        elif ':' in stripped:
            k, _, _ = stripped.partition(':')
            if k.strip() == section:
                section_idx = i
    
    # Key not found; append to section or create section
    if section_idx is not None:
        lines.append(f'  {key}: {value}')
    else:
        lines.extend([f'{section}:', f'  {key}: {value}'])
    config_path.write_text('\n'.join(lines) + '\n')


def load_all_presets() -> dict:
    """Load all presets from thresholds.json."""
    path = _find_thresholds_file()
    if path:
        try:
            with open(path) as f:
                data = json.load(f)
            return data.get('presets', {})
        except (json.JSONDecodeError, IOError):
            pass
    return {}


def load_thresholds() -> dict:
    """Load active thresholds: config preset selection → resolve from presets or custom."""
    config = load_config()
    preset_name = config.get('threshold_preset', DEFAULT_PRESET)
    
    # If custom preset, load from config/custom_thresholds.json
    if preset_name == 'custom':
        custom_path = _get_config_path().parent / 'custom_thresholds.json'
        if custom_path.exists():
            try:
                with open(custom_path) as f:
                    custom = json.load(f)
                return {
                    'ata': custom.get('ata', {}),
                    'nvme': custom.get('nvme', {}),
                }
            except (json.JSONDecodeError, IOError):
                pass
    
    presets = load_all_presets()
    
    # If preset exists, return its ata/nvme rules
    if preset_name in presets:
        preset = presets[preset_name]
        return {
            'ata': preset.get('ata', {}),
            'nvme': preset.get('nvme', {}),
        }
    
    # Fallback: try 'backblaze', then first available preset
    if DEFAULT_PRESET in presets:
        preset = presets[DEFAULT_PRESET]
        return {
            'ata': preset.get('ata', {}),
            'nvme': preset.get('nvme', {}),
        }
    
    if presets:
        first = next(iter(presets.values()))
        return {
            'ata': first.get('ata', {}),
            'nvme': first.get('nvme', {}),
        }
    
    return {}


def save_custom_thresholds(thresholds: dict):
    """Save custom thresholds to config/custom_thresholds.json."""
    custom_path = _get_config_path().parent / 'custom_thresholds.json'
    custom_path.parent.mkdir(parents=True, exist_ok=True)
    with open(custom_path, 'w') as f:
        json.dump(thresholds, f, indent=2)
        f.write('\n')


# Cache thresholds at module level (invalidated on settings change)
_thresholds = None

def get_thresholds() -> dict:
    global _thresholds
    if _thresholds is None:
        _thresholds = load_thresholds()
    return _thresholds

def invalidate_threshold_cache():
    global _thresholds
    _thresholds = None


# Cache diskmind-fetch module (loaded once, reused for all ingest/push operations)
_fetch_module = None

def get_fetch_module():
    """Load and cache the diskmind-fetch module for its library functions."""
    global _fetch_module
    if _fetch_module is None:
        fetch_path = Path(__file__).parent / 'diskmind-fetch'
        spec = importlib.util.spec_from_loader('diskmind_fetch',
            importlib.machinery.SourceFileLoader('diskmind_fetch', str(fetch_path)))
        _fetch_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(_fetch_module)
    return _fetch_module


def check_threshold(attr_value, rule: dict) -> bool:
    """Check if an attribute value exceeds a threshold rule."""
    try:
        val = float(attr_value)
    except (ValueError, TypeError):
        return False
    
    op = rule.get('op', '>')
    threshold = rule.get('value', 0)
    
    if op == '>':
        return val > threshold
    elif op == '>=':
        return val >= threshold
    elif op == '<':
        return val < threshold
    elif op == '<=':
        return val <= threshold
    elif op == '==':
        return val == threshold
    return False


def classify_disk(r: dict, history: dict = None, delta_days: float = None) -> str:
    """Classify disk status based on visible issues (respecting delta filter)."""
    issues = get_disk_issues(r, history, delta_days)
    
    # Status is determined by the filtered issues
    for issue in issues:
        if issue['level'] == 'critical':
            return 'critical'
    for issue in issues:
        if issue['level'] == 'warning':
            return 'warning'
    
    return 'ok'


# Cumulative event counters - only show if delta > 0 in selected time range
# These are counters that accumulate over time; old values aren't necessarily concerning
CUMULATIVE_EVENT_ATTRS = {
    'Command_Timeout',
    'Reported_Uncorrect',
    'UDMA_CRC_Error_Count',
    'Unsafe_Shutdowns',
    'Error_Information_Log_Entries',
    'Power_Off_Retract_Count',
}

# Critical state attributes - always show regardless of delta
# These represent current state or irreversible damage
CRITICAL_STATE_ATTRS = {
    'Reallocated_Sector_Ct',
    'Current_Pending_Sector',
    'Offline_Uncorrectable',
    'Media_and_Data_Integrity_Errors',
    'Critical_Warning',
    'Percentage_Used',
    'Available_Spare',
    'Temperature_Celsius',
    'Airflow_Temperature_Cel',
    'Temperature',
}


def decode_seagate_value(attr_name: str, raw_value) -> int:
    """Decode Seagate composite 48-bit raw values.
    
    Seagate packs multiple counters into raw values:
    - Command_Timeout (#188): low 16 bits = actual timeout count
    - Raw_Read_Error_Rate (#1): high 16 bits = error count
    - Seek_Error_Rate (#7): high 16 bits = error count
    """
    try:
        raw = int(raw_value)
    except (ValueError, TypeError):
        return 0
    
    if raw <= 65535:  # Not a composite value
        return raw
    
    if attr_name == 'Command_Timeout':
        return raw & 0xFFFF
    elif attr_name in ('Raw_Read_Error_Rate', 'Seek_Error_Rate'):
        return (raw >> 32) & 0xFFFF
    
    return raw


def get_disk_issues(r: dict, history: dict = None, delta_days: float = None) -> list[dict]:
    """Get list of threshold violations for a disk, filtered by delta time range.
    
    Args:
        r: Disk reading dict
        history: History data for this disk (with deltas)
        delta_days: Time range in days (None or >= 36500 means all time)
    
    Returns:
        List of issues that should be shown based on:
        - Critical state attrs: always shown if threshold exceeded
        - Cumulative counters: only shown if delta > 0 in time range
    """
    attrs = r.get('smart_attributes', {})
    if isinstance(attrs, str):
        try:
            attrs = json.loads(attrs)
        except:
            attrs = {}
    
    thresholds = get_thresholds()
    disk_type = (r.get('type') or '').strip()
    is_nvme = disk_type == 'NVMe'
    
    if is_nvme:
        rules = thresholds.get('nvme', {})
    else:
        rules = thresholds.get('ata', {})
    
    issues = []
    all_time = delta_days is None or delta_days >= 36500
    
    # SMART self-test failure is always critical
    if r.get('smart_status') not in ('PASSED', 'N/A', None):
        issues.append({'level': 'critical', 'text': 'SMART Failed'})
    
    def should_show_attr(attr_name: str) -> bool:
        """Determine if an attribute's issue should be shown based on delta filter."""
        # All time mode - show everything
        if all_time:
            return True
        
        # Critical state attrs always shown
        if attr_name in CRITICAL_STATE_ATTRS:
            return True
        
        # Cumulative counters - only show if delta > 0
        if attr_name in CUMULATIVE_EVENT_ATTRS:
            if history and attr_name in history:
                attr_hist = history[attr_name]
                delta = attr_hist.get('delta', 0) if isinstance(attr_hist, dict) else 0
                return delta > 0
            # No history data - don't show (can't prove it's new)
            return False
        
        # Unknown attr type - show to be safe
        return True
    
    # Check critical thresholds
    for attr_name, rule in rules.get('critical', {}).items():
        if attr_name.startswith('_'):
            continue
        val = attrs.get(attr_name)
        if val is not None:
            check_val = decode_seagate_value(attr_name, val) if not is_nvme else val
            if check_threshold(check_val, rule):
                if should_show_attr(attr_name):
                    display = rule.get('display', attr_name)
                    issues.append({'level': 'critical', 'text': f'{check_val} {display}'})
    
    # Check warning thresholds (skip if already critical for same attribute)
    critical_attrs = set(rules.get('critical', {}).keys())
    for attr_name, rule in rules.get('warning', {}).items():
        if attr_name.startswith('_'):
            continue
        # Skip if already flagged as critical
        if attr_name in critical_attrs:
            val = attrs.get(attr_name)
            if val is not None:
                check_val = decode_seagate_value(attr_name, val) if not is_nvme else val
                if check_threshold(check_val, rules['critical'][attr_name]):
                    continue
        val = attrs.get(attr_name)
        if val is not None:
            check_val = decode_seagate_value(attr_name, val) if not is_nvme else val
            if check_threshold(check_val, rule):
                if should_show_attr(attr_name):
                    display = rule.get('display', attr_name)
                    issues.append({'level': 'warning', 'text': f'{check_val} {display}'})
    
    return issues


# ---------------------------------------------------------------------------
# HTML Template
# ---------------------------------------------------------------------------


# Cache for HTML template (None = not loaded, string = cached)
_html_template = None
_dev_mode = False

def get_html_template() -> str:
    """Load HTML template from web/dashboard.html."""
    global _html_template
    if _dev_mode or _html_template is None:
        path = Path(__file__).parent.parent / 'lib' / 'dashboard.html'
        _html_template = path.read_text()
    return _html_template



# ---------------------------------------------------------------------------
# HTTP Request Handler
# ---------------------------------------------------------------------------

class SmartHTTPHandler(BaseHTTPRequestHandler):
    """HTTP request handler for diskmind."""
    
    db_path = './data/smart.db'
    fetch_script = None
    
    def log_message(self, format, *args):
        """Custom log format."""
        print(f"[{self.log_date_time_string()}] {args[0]}")
    
    def send_json(self, data: dict, status: int = 200):
        """Send JSON response."""
        self.send_response(status)
        self.send_header('Content-Type', 'application/json')
        self.end_headers()
        self.wfile.write(json.dumps(data).encode())
    
    def send_html(self, html: str, status: int = 200):
        """Send HTML response."""
        self.send_response(status)
        self.send_header('Content-Type', 'text/html; charset=utf-8')
        self.end_headers()
        self.wfile.write(html.encode())
    
    def do_GET(self):
        """Handle GET requests."""
        parsed = urlparse(self.path)
        path = parsed.path
        
        try:
            if path == '/' or path == '/index.html':
                self.send_html(get_html_template())
            
            elif path == '/api/disks':
                cleanup_old_readings(self.db_path)
                readings = get_current_readings(self.db_path)
                trends = get_trends(self.db_path)
                
                # Get configured hosts - only show these
                config = load_config()
                raw_hosts = config.get('hosts', [])
                
                # Extract IPs from method-prefixed entries
                # Formats: "push:ip", "ssh:user@ip"
                configured_hosts = []
                for h in raw_hosts:
                    rest = h
                    if h.startswith('push:'): rest = h[5:]
                    elif h.startswith('ssh:'): rest = h[4:]
                    ip = rest.split('@')[1] if '@' in rest else rest
                    configured_hosts.append(ip)
                
                # Get host status from DB
                host_status = get_host_status(self.db_path)
                
                # Filter readings to only configured hosts
                if configured_hosts:
                    readings = [r for r in readings if r['host'] in configured_hosts]
                else:
                    readings = []
                
                hosts = list(set(r['host'] for r in readings))
                
                # Get delta setting from config
                delta_preset = config.get('delta_preset', '7d')
                delta_days = DELTA_PRESETS.get(delta_preset, 7)
                
                # Load history for delta calculations
                history_data = get_disk_history(self.db_path, days=delta_days if delta_days < 36500 else 365)
                
                # Classify and parse smart_attributes
                for r in readings:
                    # Parse JSON string to dict first (needed by classify_disk)
                    if r.get('smart_attributes'):
                        try:
                            r['smart_attributes'] = json.loads(r['smart_attributes'])
                        except:
                            r['smart_attributes'] = {}
                    else:
                        r['smart_attributes'] = {}
                    
                    # Get history for this disk
                    disk_id = r.get('disk_id') or r.get('serial')
                    disk_history = history_data.get(disk_id, {})
                    
                    r['status'] = classify_disk(r, disk_history, delta_days)
                    r['issues'] = get_disk_issues(r, disk_history, delta_days)
                
                # Recalculate stats based on new status values
                stats = get_stats(readings)
                
                # Build host_status for all configured hosts
                all_host_status = {}
                for host in configured_hosts:
                    if host in host_status:
                        all_host_status[host] = host_status[host]
                    else:
                        # Host in config but never scanned
                        all_host_status[host] = {
                            'status': 'pending',
                            'message': 'Never scanned',
                            'disk_count': 0,
                            'last_attempt': None,
                            'last_success': None,
                        }
                
                # Also send thresholds to frontend for client-side awareness
                self.send_json({
                    'disks': readings,
                    'hosts': sorted(configured_hosts),
                    'host_status': all_host_status,
                    'push_attempts': get_push_attempts(self.db_path),
                    'stats': stats,
                    'trends': trends,
                    'thresholds': get_thresholds(),
                })
            
            elif path == '/api/hosts':
                hosts = get_hosts(self.db_path)
                self.send_json({'hosts': hosts})
            
            elif path == '/api/stats':
                readings = get_current_readings(self.db_path)
                stats = get_stats(readings)
                self.send_json(stats)
            
            elif path == '/api/settings':
                config = load_config()
                presets = load_all_presets()
                preset_name = config.get('threshold_preset', DEFAULT_PRESET)
                
                # Build preset summary for frontend
                preset_list = []
                for name, preset in presets.items():
                    preset_list.append({
                        'name': name,
                        'description': preset.get('_description', ''),
                        'active': name == preset_name,
                    })
                
                self.send_json({
                    'active_preset': preset_name,
                    'presets': preset_list,
                    'all_presets': presets,  # Full preset data for threshold editor
                    'thresholds': get_thresholds(),
                    'hosts': config.get('hosts', []),
                    'temp_unit': config.get('temp_unit', 'C'),
                    'host_stats': get_host_stats(self.db_path),
                    'delta_preset': config.get('delta_preset', '7d'),
                    'push_token_set': bool(config.get('push_token')),
                    'retention_days': config.get('database', {}).get('retention_days', 365),
                    'rate_limit': config.get('rate_limit', {'max_requests': 10, 'window_seconds': 60}),
                })

            elif path == '/api/history':
                params = parse_qs(parsed.query)
                disk_id = params.get('disk_id', [None])[0]
                days = float(params.get('days', [30])[0])
                history = get_disk_history(self.db_path, disk_id, days)
                self.send_json(history)
            
            else:
                self.send_response(404)
                self.end_headers()
                self.wfile.write(b'Not Found')
        
        except Exception as e:
            self.send_json({'error': str(e)}, 500)
    
    def do_POST(self):
        """Handle POST requests."""
        parsed = urlparse(self.path)
        path = parsed.path
        
        try:
            # Read request body
            content_length = int(self.headers.get('Content-Length', 0))
            body = self.rfile.read(content_length) if content_length > 0 else b''
            
            if path == '/api/settings':
                try:
                    data = json.loads(body) if body else {}
                except json.JSONDecodeError:
                    self.send_json({'error': 'Invalid JSON'}, 400)
                    return
                
                result = {}
                
                # Handle custom thresholds
                custom_thresholds = data.get('custom_thresholds')
                if custom_thresholds:
                    # Save custom thresholds to config
                    save_custom_thresholds(custom_thresholds)
                    invalidate_threshold_cache()
                    result['thresholds'] = custom_thresholds
                
                # Handle threshold preset change
                preset_name = data.get('threshold_preset')
                if preset_name:
                    if preset_name == 'custom':
                        # Just mark as custom, thresholds already saved above
                        save_config_value('threshold_preset', 'custom')
                        result['active_preset'] = 'custom'
                    else:
                        presets = load_all_presets()
                        if preset_name not in presets:
                            self.send_json({'error': f'Unknown preset: {preset_name}'}, 400)
                            return
                        save_config_value('threshold_preset', preset_name)
                        invalidate_threshold_cache()
                        result['active_preset'] = preset_name
                
                # Handle hosts change
                hosts = data.get('hosts')
                if hosts is not None:
                    # Validate: list of non-empty strings
                    hosts = [h.strip() for h in hosts if h.strip()]
                    save_config_list('hosts', hosts)
                    result['hosts'] = hosts
                
                # Handle temp_unit change
                temp_unit = data.get('temp_unit')
                if temp_unit is not None and temp_unit in ('C', 'F'):
                    save_config_value('temp_unit', temp_unit)
                    result['temp_unit'] = temp_unit
                
                # Handle delta preset change
                delta_preset = data.get('delta_preset')
                if delta_preset is not None:
                    save_config_value('delta_preset', delta_preset)
                    result['delta_preset'] = delta_preset
                
                # Handle push_token change
                push_token = data.get('push_token')
                if push_token is not None:
                    if push_token == '':
                        save_config_value('push_token', None)
                    else:
                        save_config_value('push_token', push_token)
                    result['push_token_set'] = bool(push_token)
                
                # Handle retention_days change
                retention_days = data.get('retention_days')
                if retention_days is not None:
                    save_config_subkey('database', 'retention_days', int(retention_days))
                    result['retention_days'] = int(retention_days)
                
                # Handle rate_limit change
                rate_limit = data.get('rate_limit')
                if rate_limit is not None:
                    max_req = int(rate_limit.get('max_requests', 10))
                    window = int(rate_limit.get('window_seconds', 60))
                    save_config_subkey('rate_limit', 'max_requests', max_req)
                    save_config_subkey('rate_limit', 'window_seconds', window)
                    result['rate_limit'] = {'max_requests': max_req, 'window_seconds': window}
                
                result['success'] = True
                self.send_json(result)
            
            elif path == '/api/collect':
                # Trigger data collection
                if self.fetch_script:
                    # Check for specific host in request body
                    try:
                        req_data = json.loads(body) if body else {}
                    except json.JSONDecodeError:
                        req_data = {}
                    
                    cmd = [sys.executable, self.fetch_script]
                    # If specific host requested, only fetch that one
                    if req_data.get('host'):
                        cmd.extend(['--hosts', req_data['host']])
                    
                    result = subprocess.run(
                        cmd,
                        capture_output=True,
                        text=True,
                        timeout=300
                    )
                    self.send_json({
                        'success': result.returncode == 0,
                        'output': result.stdout,
                        'error': result.stderr,
                    })
                else:
                    self.send_json({'error': 'Collect script not configured'}, 500)
            
            elif path == '/api/ingest':
                # Rate limit check
                client_ip = self.client_address[0]
                if not check_rate_limit(client_ip):
                    self.send_json({'error': 'Rate limited. Try again later.'}, 429)
                    return
                
                # Receive pushed SMART data from agents
                # Expected: CSV data in body, host in query param
                params = parse_qs(parsed.query)
                host = params.get('host', [None])[0]
                
                if not host:
                    self.send_json({'error': 'Missing host parameter'}, 400)
                    return
                
                if not body:
                    self.send_json({'error': 'Empty body'}, 400)
                    return
                
                # Validate push token if configured
                config = load_config()
                expected_token = config.get('push_token')
                if expected_token:
                    auth_header = self.headers.get('Authorization', '')
                    provided_token = ''
                    if auth_header.startswith('Bearer '):
                        provided_token = auth_header[7:]
                    if provided_token != expected_token:
                        self.send_json({'error': 'Invalid or missing push token'}, 401)
                        return
                
                # Check host is configured for push
                raw_hosts = config.get('hosts', [])
                host_method = None
                for h in raw_hosts:
                    rest = h
                    if h.startswith('push:'): method, rest = 'push', h[5:]
                    elif h.startswith('ssh:'): method, rest = 'ssh', h[4:]
                    else: method = 'ssh'
                    ip = rest.split('@')[1] if '@' in rest else rest
                    if ip == host:
                        host_method = method
                        break
                
                if host_method is None:
                    # Log the push attempt from unknown host
                    try:
                        fm = get_fetch_module()
                        conn = fm.init_database(self.db_path)
                        conn.execute('''
                            INSERT INTO push_attempts (host, last_attempt, attempts, reason)
                            VALUES (?, ?, 1, 'unknown')
                            ON CONFLICT(host) DO UPDATE SET
                                last_attempt = excluded.last_attempt,
                                attempts = push_attempts.attempts + 1,
                                reason = 'unknown'
                        ''', (host, datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')))
                        conn.commit()
                        conn.close()
                    except Exception as e:
                        print(f"[warn] Failed to log push attempt for {host}: {e}", file=sys.stderr)
                    self.send_json({'error': f'Unknown host: {host}'}, 403)
                    return
                
                if host_method != 'push':
                    # Log the push attempt so frontend can show a hint
                    try:
                        fm = get_fetch_module()
                        conn = fm.init_database(self.db_path)
                        conn.execute('''
                            INSERT INTO push_attempts (host, last_attempt, attempts, reason)
                            VALUES (?, ?, 1, 'ssh')
                            ON CONFLICT(host) DO UPDATE SET
                                last_attempt = excluded.last_attempt,
                                attempts = push_attempts.attempts + 1,
                                reason = 'ssh'
                        ''', (host, datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')))
                        conn.commit()
                        conn.close()
                    except Exception as e:
                        print(f"[warn] Failed to log push attempt for {host}: {e}", file=sys.stderr)
                    self.send_json({'error': f'Host {host} is configured for SSH, not push'}, 403)
                    return
                
                try:
                    # Parse CSV and store using cached fetch module
                    fetch_module = get_fetch_module()
                    csv_data = body.decode('utf-8')
                    readings = fetch_module.parse_csv(csv_data, host=host)
                    
                    if not readings:
                        self.send_json({'error': 'No valid readings in CSV'}, 400)
                        return
                    
                    # Store in database
                    conn = fetch_module.init_database(self.db_path)
                    timestamp = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')
                    fetch_module.store_readings(conn, readings, timestamp)
                    fetch_module.update_host_status(conn, host, 'ok', None, len(readings), timestamp)
                    # Clear any prior push attempts for this host
                    conn.execute('DELETE FROM push_attempts WHERE host = ?', (host,))
                    conn.commit()
                    conn.close()
                    
                    self.send_json({
                        'success': True,
                        'host': host,
                        'disks': len(readings),
                        'timestamp': timestamp,
                    })
                    
                except Exception as e:
                    self.send_json({'error': f'Failed to process data: {str(e)}'}, 500)
            
            elif path == '/api/push-approve':
                try:
                    data = json.loads(body) if body else {}
                except json.JSONDecodeError:
                    self.send_json({'error': 'Invalid JSON'}, 400)
                    return
                
                host = data.get('host')
                action = data.get('action')  # 'accept' or 'dismiss'
                
                if not host or action not in ('accept', 'dismiss'):
                    self.send_json({'error': 'Missing host or invalid action'}, 400)
                    return
                
                if action == 'accept':
                    # Add host as push:IP to config
                    config = load_config()
                    hosts = config.get('hosts', [])
                    entry = f'push:{host}'
                    if entry not in hosts:
                        hosts.append(entry)
                        save_config_list('hosts', hosts)
                
                # Clear push_attempts entry
                try:
                    fm = get_fetch_module()
                    conn = fm.init_database(self.db_path)
                    conn.execute('DELETE FROM push_attempts WHERE host = ?', (host,))
                    conn.commit()
                    conn.close()
                except Exception as e:
                    print(f"[warn] Failed to clear push attempts for {host}: {e}", file=sys.stderr)
                
                self.send_json({'success': True, 'action': action, 'host': host})
            
            else:
                self.send_response(404)
                self.end_headers()
        
        except Exception as e:
            self.send_json({'error': str(e)}, 500)


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(description='diskmind Web Server')
    parser.add_argument('-p', '--port', type=int, default=8080, help='Port (default: 8080)')
    parser.add_argument('--host', default='0.0.0.0', help='Host (default: 0.0.0.0)')
    parser.add_argument('--db', default='./data/smart.db', help='Database path')
    parser.add_argument('-c', '--config', default='config/config.yaml', help='Config file')
    parser.add_argument('--dev', action='store_true', help='Dev mode: reload HTML template on every request')
    
    args = parser.parse_args()
    
    # Load config
    if HAS_YAML and os.path.exists(args.config):
        with open(args.config) as f:
            config = yaml.safe_load(f)
        args.db = config.get('database', {}).get('path', args.db)
    
    # Initialize database (create if missing)
    if not os.path.exists(args.db):
        print(f"Database not found: {args.db} — creating empty database")
        fm = get_fetch_module()
        conn = fm.init_database(args.db)
        conn.close()
    
    # Set handler config
    SmartHTTPHandler.db_path = args.db
    SmartHTTPHandler.fetch_script = str(Path(__file__).parent / 'diskmind-fetch')
    
    # Dev mode: reload template on every request
    if args.dev:
        global _dev_mode
        _dev_mode = True
    
    # Start server
    server = ThreadingHTTPServer((args.host, args.port), SmartHTTPHandler)
    
    print(f"diskmind Web Server")
    print(f"=" * 40)
    print(f"Database: {args.db}")
    print(f"URL: http://{args.host}:{args.port}")
    if _dev_mode:
        print(f"Mode: dev (template reloads on every request)")
    print(f"")
    print(f"Press Ctrl+C to stop")
    print()
    
    try:
        server.serve_forever()
    except KeyboardInterrupt:
        print("\nShutting down...")
        server.shutdown()


if __name__ == '__main__':
    main()
