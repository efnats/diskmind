#!/usr/bin/env python3
"""
diskmind - Web Server
Serves live reports from SQLite database.
"""
import sys
sys.dont_write_bytecode = True

import argparse
import hmac
import importlib.machinery
import importlib.util
import json
import os
import sqlite3
import subprocess
import sys
import threading
import time
from datetime import datetime, timezone
from http.server import HTTPServer, BaseHTTPRequestHandler, ThreadingHTTPServer
from pathlib import Path
from urllib.parse import urlparse, parse_qs

# Shared library
sys.path.insert(0, str(Path(__file__).resolve().parent.parent / 'lib'))
from diskmind_core import (VERSION, parse_simple_yaml, classify_disk,
                          get_disk_issues, check_threshold, decode_seagate_value,
                          generate_alerts, send_notifications, _send_webhook,
                          _format_payload,
                          CUMULATIVE_EVENT_ATTRS, CRITICAL_STATE_ATTRS)

from collections import defaultdict

# ---------------------------------------------------------------------------
# Rate Limiting
# ---------------------------------------------------------------------------

_rate_limit_requests = defaultdict(list)

def check_rate_limit(ip: str) -> bool:
    """Return True if request is allowed, False if rate limited."""
    config = load_config()
    rate_config = config.get('rate_limit', {})
    max_requests = rate_config.get('max_requests', 10)
    window_seconds = rate_config.get('window_seconds', 60)
    
    # Disabled if max_requests is 0
    if max_requests <= 0:
        return True
    
    now = time.time()
    # Remove timestamps outside the window
    _rate_limit_requests[ip] = [t for t in _rate_limit_requests[ip] if now - t < window_seconds]
    # Check limit
    if len(_rate_limit_requests[ip]) >= max_requests:
        return False
    _rate_limit_requests[ip].append(now)
    return True

# ---------------------------------------------------------------------------
# Database Functions
# ---------------------------------------------------------------------------

def get_db_connection(db_path: str) -> sqlite3.Connection:
    """Get database connection with row factory."""
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    return conn


# Track last cleanup time
_last_cleanup = 0


def cleanup_old_readings(db_path: str):
    """Remove readings older than retention_days. Runs at most once per hour."""
    global _last_cleanup
    now = time.time()
    if now - _last_cleanup < 3600:
        return
    _last_cleanup = now
    
    config = load_config()
    retention_days = config.get('database', {}).get('retention_days', 365)
    
    conn = get_db_connection(db_path)
    
    if retention_days > 0:
        fm = get_fetch_module()
        fm.cleanup_old_data(conn, retention_days)
    
    # Clean old alerts based on panel retention setting
    alert_retention = config.get('panel', {}).get('alert_retention_days', 14)
    try:
        alert_retention = int(alert_retention)
    except (ValueError, TypeError):
        alert_retention = 14
    if alert_retention > 0:
        conn.execute(
            "DELETE FROM alerts WHERE timestamp < datetime('now', ? || ' days')",
            (str(-alert_retention),))
        conn.execute(
            "DELETE FROM notification_log WHERE timestamp < datetime('now', ? || ' days')",
            (str(-alert_retention),))
        conn.commit()
    
    conn.close()


# Delta preset mappings (preset name -> days)
DELTA_PRESETS = {
    '1h': 1/24,
    '24h': 1,
    '7d': 7,
    '30d': 30,
    '90d': 90,
    'all': 36500,
}


def get_current_readings(db_path: str) -> list[dict]:
    """Get most recent reading for each disk."""
    conn = get_db_connection(db_path)
    cursor = conn.cursor()
    
    # Get latest reading per disk_id (window function, requires SQLite â‰¥ 3.25)
    cursor.execute('''
        SELECT * FROM (
            SELECT *, ROW_NUMBER() OVER (PARTITION BY disk_id ORDER BY timestamp DESC) AS rn
            FROM readings
        ) WHERE rn = 1
        ORDER BY host, device
    ''')
    
    results = [dict(row) for row in cursor.fetchall()]
    conn.close()
    return results


def get_trends(db_path: str, days: int = 30) -> dict:
    """Get sector trends for each disk (reallocated sectors change over time)."""
    conn = get_db_connection(db_path)
    cursor = conn.cursor()
    
    # Get all readings from last N days
    cursor.execute('''
        SELECT disk_id, smart_attributes, timestamp
        FROM readings
        WHERE timestamp > datetime('now', ?)
        ORDER BY disk_id, timestamp
    ''', (f'-{days} days',))
    
    # Group by disk_id and extract reallocated sectors
    by_disk = {}
    for row in cursor.fetchall():
        disk_id = row['disk_id']
        if disk_id not in by_disk:
            by_disk[disk_id] = []
        
        attrs = row['smart_attributes']
        if isinstance(attrs, str):
            try:
                attrs = json.loads(attrs)
            except:
                attrs = {}
        
        realloc = int(attrs.get('Reallocated_Sector_Ct', 0) or 0)
        by_disk[disk_id].append(realloc)
    
    # Calculate trends
    trends = {}
    for disk_id, values in by_disk.items():
        if len(values) >= 2:
            delta = values[-1] - values[0]
            if delta != 0:
                trends[disk_id] = {
                    'old': values[0],
                    'new': values[-1],
                    'delta': delta
                }
    
    conn.close()
    return trends


def get_disk_history(db_path: str, disk_id: str = None, days: int = 30) -> dict:
    """Get historical attribute values for sparkline rendering.
    Returns {disk_id: {attr_name: [{timestamp, value}, ...], ...}, ...}
    """
    conn = get_db_connection(db_path)
    cursor = conn.cursor()

    if disk_id:
        cursor.execute('''
            SELECT disk_id, timestamp, smart_attributes, type
            FROM readings
            WHERE disk_id = ? AND timestamp > datetime('now', ?)
            ORDER BY timestamp
        ''', (disk_id, f'-{days} days'))
    else:
        cursor.execute('''
            SELECT disk_id, timestamp, smart_attributes, type
            FROM readings
            WHERE timestamp > datetime('now', ?)
            ORDER BY disk_id, timestamp
        ''', (f'-{days} days',))

    history = {}
    for row in cursor.fetchall():
        did = row['disk_id']
        if did not in history:
            history[did] = {'_type': row['type'], '_timestamps': []}

        ts = row['timestamp']
        history[did]['_timestamps'].append(ts)

        attrs = row['smart_attributes']
        if isinstance(attrs, str):
            try:
                attrs = json.loads(attrs)
            except:
                attrs = {}

        for attr_name, value in attrs.items():
            if attr_name not in history[did]:
                history[did][attr_name] = []
            try:
                history[did][attr_name].append({'t': ts, 'v': float(value)})
            except (ValueError, TypeError):
                pass

    # Load true first-seen timestamps (not affected by query window or retention)
    first_seen = {}
    try:
        cursor.execute('SELECT disk_id, first_seen FROM disk_first_seen')
        for row in cursor.fetchall():
            first_seen[row['disk_id']] = row['first_seen']
    except Exception:
        pass  # Table may not exist yet (old schema)

    conn.close()

    # Compute per-attribute summary: current, delta, points array
    result = {}
    for did, attr_data in history.items():
        disk_type = attr_data.pop('_type', None)
        timestamps = attr_data.pop('_timestamps', [])
        result[did] = {
            '_type': disk_type,
            '_readings': len(set(timestamps)),
            '_first': first_seen.get(did) or (timestamps[0] if timestamps else None),
            '_last': timestamps[-1] if timestamps else None,
        }
        for attr_name, points in attr_data.items():
            if len(points) < 1:
                continue
            values = [p['v'] for p in points]
            result[did][attr_name] = {
                'points': values,
                'current': values[-1],
                'delta': values[-1] - values[0] if len(values) >= 2 else 0,
                'min': min(values),
                'max': max(values),
            }

    # Include first_seen for ALL disks (independent of time window)
    result['_first_seen'] = first_seen

    return result


def get_hosts(db_path: str) -> list[str]:
    """Get list of all hosts."""
    conn = get_db_connection(db_path)
    cursor = conn.cursor()
    cursor.execute('SELECT DISTINCT host FROM readings ORDER BY host')
    hosts = [row['host'] for row in cursor.fetchall()]
    conn.close()
    return hosts


def get_host_status(db_path: str) -> dict:
    """Get host status from host_status table."""
    conn = get_db_connection(db_path)
    cursor = conn.cursor()
    
    # Check if table exists
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='host_status'")
    if not cursor.fetchone():
        conn.close()
        return {}
    
    cursor.execute('''
        SELECT host, status, message, disk_count, last_attempt, last_success
        FROM host_status
    ''')
    result = {}
    for row in cursor.fetchall():
        result[row['host']] = {
            'status': row['status'],
            'message': row['message'],
            'disk_count': row['disk_count'],
            'last_attempt': row['last_attempt'],
            'last_success': row['last_success'],
        }
    conn.close()
    return result


def get_push_attempts(db_path: str) -> dict:
    """Get rejected push attempts (hosts trying to push while configured as SSH or unknown)."""
    conn = get_db_connection(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='push_attempts'")
    if not cursor.fetchone():
        conn.close()
        return {}
    cursor.execute('SELECT host, last_attempt, attempts, reason FROM push_attempts')
    result = {}
    for row in cursor.fetchall():
        result[row['host']] = {
            'last_attempt': row['last_attempt'],
            'attempts': row['attempts'],
            'reason': row['reason'],
        }
    conn.close()
    return result


def get_host_stats(db_path: str) -> dict:
    """Get per-host last scan time and disk count."""
    conn = get_db_connection(db_path)
    cursor = conn.cursor()
    cursor.execute('''
        SELECT host,
               MAX(timestamp) as last_seen,
               COUNT(DISTINCT serial) as disk_count
        FROM readings
        GROUP BY host
    ''')
    stats = {}
    for row in cursor.fetchall():
        stats[row['host']] = {
            'last_seen': row['last_seen'],
            'disk_count': row['disk_count'],
        }
    conn.close()
    return stats


def get_stats(readings: list[dict]) -> dict:
    """Calculate summary statistics. Expects r['status'] to be set on each reading."""
    total = len(readings)
    critical = 0
    warning = 0
    healthy = 0
    total_capacity = 0
    
    for r in readings:
        total_capacity += r.get('capacity_bytes') or 0
        
        status = r.get('status', 'ok')
        if status == 'critical':
            critical += 1
        elif status == 'warning':
            warning += 1
        else:
            healthy += 1
    
    return {
        'total': total,
        'critical': critical,
        'warning': warning,
        'healthy': healthy,
        'total_capacity_tb': round(total_capacity / 1e12, 1) if total_capacity else 0,
    }


# ---------------------------------------------------------------------------
# Threshold-based Health Classification
# ---------------------------------------------------------------------------

DEFAULT_PRESET = 'backblaze'

def _find_thresholds_file() -> Path | None:
    """Locate config/thresholds.json."""
    base_dir = Path(__file__).parent.parent
    candidates = [
        base_dir / 'config' / 'thresholds.json',
        base_dir / 'lib' / 'thresholds.json',
        base_dir / 'thresholds.json',
        Path(__file__).parent / 'thresholds.json',
    ]
    for path in candidates:
        if path.exists():
            return path
    return None


def _get_config_path() -> Path:
    """Get path to config/config.yaml."""
    base_dir = Path(__file__).parent.parent
    return base_dir / 'config' / 'config.yaml'


_config_cache = None
_config_mtime = 0

def load_config() -> dict:
    """Load user config from config/config.yaml (cached, re-reads on file change)."""
    global _config_cache, _config_mtime
    config_path = _get_config_path()
    try:
        mtime = config_path.stat().st_mtime
        if _config_cache is not None and mtime == _config_mtime:
            return _config_cache
        with open(config_path) as f:
            _config_cache = parse_simple_yaml(f.read())
        _config_mtime = mtime
        return _config_cache
    except (OSError, IOError):
        return {}


def save_config_value(key: str, value):
    """Update a single top-level key in config/config.yaml, preserving comments.
    If value is None, removes the key from the config file."""
    config_path = _get_config_path()
    config_path.parent.mkdir(parents=True, exist_ok=True)
    
    if config_path.exists():
        lines = config_path.read_text().splitlines()
    else:
        lines = []
    
    # Find and replace existing key, or append
    found = False
    for i, line in enumerate(lines):
        stripped = line.strip()
        if stripped.startswith('#'):
            continue
        if ':' in stripped:
            k, _, _ = stripped.partition(':')
            if k.strip() == key:
                if value is None:
                    lines.pop(i)
                else:
                    lines[i] = f'{key}: {value}'
                found = True
                break
    if not found and value is not None:
        lines.append(f'{key}: {value}')
    
    config_path.write_text('\n'.join(lines) + '\n')


def save_config_list(key: str, items: list):
    """Update a top-level list key in config/config.yaml, preserving comments."""
    config_path = _get_config_path()
    config_path.parent.mkdir(parents=True, exist_ok=True)
    
    if config_path.exists():
        lines = config_path.read_text().splitlines()
    else:
        lines = []
    
    # Find key, remove old list items, insert new ones
    key_idx = None
    remove_end = None
    for i, line in enumerate(lines):
        stripped = line.strip()
        if stripped.startswith('#'):
            continue
        if key_idx is not None:
            # We're inside the list â€” consume indented lines
            indent = len(line) - len(line.lstrip())
            if indent > 0 and (stripped.startswith('- ') or stripped.startswith('#')):
                remove_end = i + 1
            else:
                break
        elif ':' in stripped:
            k, _, _ = stripped.partition(':')
            if k.strip() == key:
                key_idx = i
                remove_end = i + 1
    
    new_lines = [f'  - {item}' for item in items]
    
    if key_idx is not None:
        lines[key_idx:remove_end] = [f'{key}:'] + new_lines
    else:
        lines.append(f'{key}:')
        lines.extend(new_lines)
    
    config_path.write_text('\n'.join(lines) + '\n')


def save_config_subkey(section: str, key: str, value):
    """Update a nested key (e.g. ssh.user) in config/config.yaml."""
    config_path = _get_config_path()
    config_path.parent.mkdir(parents=True, exist_ok=True)
    
    if config_path.exists():
        lines = config_path.read_text().splitlines()
    else:
        lines = []
    
    section_idx = None
    for i, line in enumerate(lines):
        stripped = line.strip()
        if stripped.startswith('#'):
            continue
        if section_idx is not None:
            indent = len(line) - len(line.lstrip())
            if indent > 0 and ':' in stripped:
                k, _, _ = stripped.partition(':')
                if k.strip() == key:
                    lines[i] = f'  {key}: {value}'
                    config_path.write_text('\n'.join(lines) + '\n')
                    return
            elif indent == 0 and stripped:
                # Past the section, insert before this line
                lines.insert(i, f'  {key}: {value}')
                config_path.write_text('\n'.join(lines) + '\n')
                return
        elif ':' in stripped:
            k, _, _ = stripped.partition(':')
            if k.strip() == section:
                section_idx = i
    
    # Key not found; append to section or create section
    if section_idx is not None:
        lines.append(f'  {key}: {value}')
    else:
        lines.extend([f'{section}:', f'  {key}: {value}'])
    config_path.write_text('\n'.join(lines) + '\n')


def load_all_presets() -> dict:
    """Load all presets from thresholds.json."""
    path = _find_thresholds_file()
    if path:
        try:
            with open(path) as f:
                data = json.load(f)
            return data.get('presets', {})
        except (json.JSONDecodeError, IOError):
            pass
    return {}


def load_thresholds() -> dict:
    """Load active thresholds: config preset selection â†’ resolve from presets or custom."""
    config = load_config()
    preset_name = config.get('threshold_preset', DEFAULT_PRESET)
    
    # If custom preset, load from config/custom_thresholds.json
    if preset_name == 'custom':
        custom_path = _get_config_path().parent / 'custom_thresholds.json'
        if custom_path.exists():
            try:
                with open(custom_path) as f:
                    custom = json.load(f)
                return {
                    'ata': custom.get('ata', {}),
                    'nvme': custom.get('nvme', {}),
                }
            except (json.JSONDecodeError, IOError):
                pass
    
    presets = load_all_presets()
    
    # If preset exists, return its ata/nvme rules
    if preset_name in presets:
        preset = presets[preset_name]
        return {
            'ata': preset.get('ata', {}),
            'nvme': preset.get('nvme', {}),
        }
    
    # Fallback: try 'backblaze', then first available preset
    if DEFAULT_PRESET in presets:
        preset = presets[DEFAULT_PRESET]
        return {
            'ata': preset.get('ata', {}),
            'nvme': preset.get('nvme', {}),
        }
    
    if presets:
        first = next(iter(presets.values()))
        return {
            'ata': first.get('ata', {}),
            'nvme': first.get('nvme', {}),
        }
    
    return {}


def save_custom_thresholds(thresholds: dict):
    """Save custom thresholds to config/custom_thresholds.json."""
    custom_path = _get_config_path().parent / 'custom_thresholds.json'
    custom_path.parent.mkdir(parents=True, exist_ok=True)
    with open(custom_path, 'w') as f:
        json.dump(thresholds, f, indent=2)
        f.write('\n')


# Cache thresholds at module level (invalidated on settings change)
_thresholds = None

def get_thresholds() -> dict:
    global _thresholds
    if _thresholds is None:
        _thresholds = load_thresholds()
    return _thresholds

def invalidate_threshold_cache():
    global _thresholds
    _thresholds = None


# Cache diskmind-fetch module (loaded once, reused for all ingest/push operations)
_fetch_module = None

def get_fetch_module():
    """Load and cache the diskmind-fetch module for its library functions."""
    global _fetch_module
    if _fetch_module is None:
        fetch_path = Path(__file__).parent / 'diskmind-fetch'
        spec = importlib.util.spec_from_loader('diskmind_fetch',
            importlib.machinery.SourceFileLoader('diskmind_fetch', str(fetch_path)))
        _fetch_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(_fetch_module)
    return _fetch_module





# ---------------------------------------------------------------------------
# HTML Template
# ---------------------------------------------------------------------------


# Cache for HTML template (None = not loaded, string = cached)
_html_template = None
_dev_mode = False

def get_html_template() -> str:
    """Load HTML template from lib/dashboard.html with version injection."""
    global _html_template
    if _dev_mode or _html_template is None:
        path = Path(__file__).parent.parent / 'lib' / 'dashboard.html'
        _html_template = path.read_text().replace('{{VERSION}}', VERSION)
    return _html_template



# ---------------------------------------------------------------------------
# HTTP Request Handler
# ---------------------------------------------------------------------------

class SmartHTTPHandler(BaseHTTPRequestHandler):
    """HTTP request handler for diskmind."""
    
    db_path = './data/diskmind.db'
    fetch_script = None
    
    def log_message(self, format, *args):
        """Custom log format."""
        print(f"[{self.log_date_time_string()}] {args[0]}")
    
    def send_json(self, data: dict, status: int = 200):
        """Send JSON response."""
        self.send_response(status)
        self.send_header('Content-Type', 'application/json')
        self.end_headers()
        self.wfile.write(json.dumps(data).encode())
    
    def send_html(self, html: str, status: int = 200):
        """Send HTML response."""
        self.send_response(status)
        self.send_header('Content-Type', 'text/html; charset=utf-8')
        self.end_headers()
        self.wfile.write(html.encode())
    
    def do_GET(self):
        """Handle GET requests."""
        parsed = urlparse(self.path)
        path = parsed.path
        
        try:
            if path == '/' or path == '/index.html':
                self.send_html(get_html_template())
            
            elif path == '/api/disks':
                cleanup_old_readings(self.db_path)
                readings = get_current_readings(self.db_path)
                trends = get_trends(self.db_path)
                
                # Get configured hosts - only show these
                config = load_config()
                raw_hosts = config.get('hosts', [])
                
                # Extract IPs from method-prefixed entries
                # Formats: "push:ip", "ssh:user@ip"
                configured_hosts = []
                for h in raw_hosts:
                    rest = h
                    if h.startswith('push:'): rest = h[5:]
                    elif h.startswith('ssh:'): rest = h[4:]
                    ip = rest.split('@')[1] if '@' in rest else rest
                    configured_hosts.append(ip)
                
                # Get host status from DB
                host_status = get_host_status(self.db_path)
                
                # Filter readings to only configured hosts
                if configured_hosts:
                    readings = [r for r in readings if r['host'] in configured_hosts]
                else:
                    readings = []
                
                hosts = list(set(r['host'] for r in readings))
                
                # Get delta setting from config
                delta_preset = config.get('delta_preset', '7d')
                delta_days = DELTA_PRESETS.get(delta_preset, 7)
                
                # Load history for delta calculations
                history_data = get_disk_history(self.db_path, days=delta_days if delta_days < 36500 else 365)
                
                # Classify and parse smart_attributes
                thresholds = get_thresholds()
                for r in readings:
                    # Parse JSON string to dict first (needed by classify_disk)
                    if r.get('smart_attributes'):
                        try:
                            r['smart_attributes'] = json.loads(r['smart_attributes'])
                        except:
                            r['smart_attributes'] = {}
                    else:
                        r['smart_attributes'] = {}
                    
                    # Get history for this disk
                    disk_id = r.get('disk_id') or r.get('serial')
                    disk_history = history_data.get(disk_id, {})
                    
                    r['status'] = classify_disk(r, thresholds, disk_history, delta_days)
                    r['issues'] = get_disk_issues(r, thresholds, disk_history, delta_days)
                
                # Recalculate stats based on new status values
                stats = get_stats(readings)
                
                # Build host_status for all configured hosts
                all_host_status = {}
                for host in configured_hosts:
                    if host in host_status:
                        all_host_status[host] = host_status[host]
                    else:
                        # Host in config but never scanned
                        all_host_status[host] = {
                            'status': 'pending',
                            'message': 'Never scanned',
                            'disk_count': 0,
                            'last_attempt': None,
                            'last_success': None,
                        }
                
                # Also send thresholds to frontend for client-side awareness
                self.send_json({
                    'disks': readings,
                    'hosts': sorted(configured_hosts),
                    'host_status': all_host_status,
                    'push_attempts': get_push_attempts(self.db_path),
                    'stats': stats,
                    'trends': trends,
                    'thresholds': thresholds,
                })
            
            elif path == '/api/hosts':
                hosts = get_hosts(self.db_path)
                self.send_json({'hosts': hosts})
            
            elif path == '/api/stats':
                readings = get_current_readings(self.db_path)
                thresholds = get_thresholds()
                for r in readings:
                    if r.get('smart_attributes'):
                        try:
                            r['smart_attributes'] = json.loads(r['smart_attributes'])
                        except Exception:
                            r['smart_attributes'] = {}
                    r['status'] = classify_disk(r, thresholds)
                stats = get_stats(readings)
                self.send_json(stats)
            
            elif path == '/api/alerts':
                params = parse_qs(parsed.query)
                limit = int(params.get('limit', ['100'])[0])
                config = load_config()
                panel = config.get('panel') or {}
                retention = float(panel.get('alert_retention_days', 14))
                conn = get_db_connection(self.db_path)
                cursor = conn.cursor()
                cursor.execute('''
                    SELECT id, disk_id, host, timestamp, alert_type, severity,
                           attribute, old_value, new_value, message, acknowledged
                    FROM alerts
                    WHERE timestamp > datetime('now', ?)
                    ORDER BY timestamp DESC LIMIT ?
                ''', (f'-{retention} days', limit))
                alerts = []
                for row in cursor.fetchall():
                    alerts.append({
                        'id': row['id'],
                        'disk_id': row['disk_id'],
                        'host': row['host'],
                        'timestamp': row['timestamp'],
                        'alert_type': row['alert_type'],
                        'severity': row['severity'],
                        'attribute': row['attribute'],
                        'old_value': row['old_value'],
                        'new_value': row['new_value'],
                        'message': row['message'],
                        'acknowledged': bool(row['acknowledged']),
                    })
                # Count unacknowledged (within retention window)
                cursor.execute('''
                    SELECT COUNT(*) FROM alerts
                    WHERE acknowledged = 0
                      AND timestamp > datetime('now', ?)
                ''', (f'-{retention} days',))
                unread = cursor.fetchone()[0]
                # Highest severity among unread alerts
                cursor.execute('''
                    SELECT severity FROM alerts
                    WHERE acknowledged = 0
                      AND timestamp > datetime('now', ?)
                    ORDER BY CASE severity
                        WHEN 'critical' THEN 3
                        WHEN 'warning' THEN 2
                        WHEN 'info' THEN 1
                        ELSE 0 END DESC
                    LIMIT 1
                ''', (f'-{retention} days',))
                row = cursor.fetchone()
                max_sev = row[0] if row else None
                conn.close()
                self.send_json({'alerts': alerts, 'unread': unread, 'max_severity': max_sev})
            
            elif path == '/api/settings':
                config = load_config()
                presets = load_all_presets()
                preset_name = config.get('threshold_preset', DEFAULT_PRESET)
                
                # Build preset summary for frontend
                preset_list = []
                for name, preset in presets.items():
                    preset_list.append({
                        'name': name,
                        'description': preset.get('_description', ''),
                        'active': name == preset_name,
                    })
                
                self.send_json({
                    'active_preset': preset_name,
                    'presets': preset_list,
                    'all_presets': presets,  # Full preset data for threshold editor
                    'thresholds': get_thresholds(),
                    'hosts': config.get('hosts', []),
                    'temp_unit': config.get('temp_unit', 'C'),
                    'host_stats': get_host_stats(self.db_path),
                    'delta_preset': config.get('delta_preset', '7d'),
                    'push_token_set': bool(config.get('push_token')),
                    'retention_days': config.get('database', {}).get('retention_days', 365),
                    'rate_limit': config.get('rate_limit', {'max_requests': 10, 'window_seconds': 60}),
                    'notifications': config.get('notifications', {}),
                    'panel': config.get('panel', {}),
                    'webhook_urls': config.get('webhook_urls', []),
                })

            elif path == '/api/history':
                params = parse_qs(parsed.query)
                disk_id = params.get('disk_id', [None])[0]
                days = float(params.get('days', [30])[0])
                history = get_disk_history(self.db_path, disk_id, days)
                self.send_json(history)
            
            elif path == '/api/webhook-status':
                conn = get_db_connection(self.db_path)
                cursor = conn.cursor()
                cursor.execute('''
                    SELECT channel, success, error, MAX(timestamp) as last_ts
                    FROM notification_log
                    WHERE channel LIKE 'http%'
                    GROUP BY channel
                ''')
                rows = cursor.fetchall()
                conn.close()
                status = {}
                for ch, success, error, last_ts in rows:
                    status[ch] = {
                        'success': bool(success),
                        'error': error or '',
                        'timestamp': last_ts,
                    }
                self.send_json(status)
            
            else:
                self.send_response(404)
                self.end_headers()
                self.wfile.write(b'Not Found')
        
        except Exception as e:
            self.send_json({'error': str(e)}, 500)
    
    def do_POST(self):
        """Handle POST requests."""
        parsed = urlparse(self.path)
        path = parsed.path
        
        try:
            # Read request body
            content_length = int(self.headers.get('Content-Length', 0))
            body = self.rfile.read(content_length) if content_length > 0 else b''
            
            if path == '/api/settings':
                try:
                    data = json.loads(body) if body else {}
                except json.JSONDecodeError:
                    self.send_json({'error': 'Invalid JSON'}, 400)
                    return
                
                result = {}
                
                # Handle custom thresholds
                custom_thresholds = data.get('custom_thresholds')
                if custom_thresholds:
                    # Save custom thresholds to config
                    save_custom_thresholds(custom_thresholds)
                    invalidate_threshold_cache()
                    result['thresholds'] = custom_thresholds
                
                # Handle threshold preset change
                preset_name = data.get('threshold_preset')
                if preset_name:
                    if preset_name == 'custom':
                        # Just mark as custom, thresholds already saved above
                        save_config_value('threshold_preset', 'custom')
                        invalidate_threshold_cache()
                        result['active_preset'] = 'custom'
                    else:
                        presets = load_all_presets()
                        if preset_name not in presets:
                            self.send_json({'error': f'Unknown preset: {preset_name}'}, 400)
                            return
                        save_config_value('threshold_preset', preset_name)
                        invalidate_threshold_cache()
                        result['active_preset'] = preset_name
                
                # Handle hosts change
                hosts = data.get('hosts')
                if hosts is not None:
                    # Validate: list of non-empty strings
                    hosts = [h.strip() for h in hosts if h.strip()]
                    save_config_list('hosts', hosts)
                    result['hosts'] = hosts
                
                # Handle temp_unit change
                temp_unit = data.get('temp_unit')
                if temp_unit is not None and temp_unit in ('C', 'F'):
                    save_config_value('temp_unit', temp_unit)
                    result['temp_unit'] = temp_unit
                
                # Handle delta preset change
                delta_preset = data.get('delta_preset')
                if delta_preset is not None:
                    save_config_value('delta_preset', delta_preset)
                    result['delta_preset'] = delta_preset
                
                # Handle push_token change
                push_token = data.get('push_token')
                if push_token is not None:
                    if push_token == '':
                        save_config_value('push_token', None)
                    else:
                        save_config_value('push_token', push_token)
                    result['push_token_set'] = bool(push_token)
                
                # Handle retention_days change
                retention_days = data.get('retention_days')
                if retention_days is not None:
                    save_config_subkey('database', 'retention_days', int(retention_days))
                    result['retention_days'] = int(retention_days)
                
                # Handle rate_limit change
                rate_limit = data.get('rate_limit')
                if rate_limit is not None:
                    max_req = int(rate_limit.get('max_requests', 10))
                    window = int(rate_limit.get('window_seconds', 60))
                    save_config_subkey('rate_limit', 'max_requests', max_req)
                    save_config_subkey('rate_limit', 'window_seconds', window)
                    result['rate_limit'] = {'max_requests': max_req, 'window_seconds': window}
                
                # Handle notifications change
                notifications = data.get('notifications')
                if notifications is not None:
                    for nkey in ('min_severity', 'cooldown_minutes',
                                 'include_recovery'):
                        if nkey in notifications:
                            val = notifications[nkey]
                            # Convert booleans to yaml-friendly strings
                            if isinstance(val, bool):
                                val = 'true' if val else 'false'
                            save_config_subkey('notifications', nkey, val if val else '')
                    result['notifications'] = notifications
                
                # Handle panel settings change
                panel = data.get('panel')
                if panel is not None:
                    for pkey in ('alert_retention_days', 'alert_sound'):
                        if pkey in panel:
                            val = panel[pkey]
                            if isinstance(val, bool):
                                val = 'true' if val else 'false'
                            save_config_subkey('panel', pkey, val if val else '')
                    result['panel'] = panel
                
                # Handle webhook_urls change (top-level list)
                webhook_urls = data.get('webhook_urls')
                if webhook_urls is not None:
                    urls = [u.strip() for u in webhook_urls if u.strip()]
                    save_config_list('webhook_urls', urls)
                    result['webhook_urls'] = urls
                
                result['success'] = True
                self.send_json(result)
            
            elif path == '/api/test-webhook':
                try:
                    req = json.loads(body) if body else {}
                except json.JSONDecodeError:
                    self.send_json({'error': 'Invalid JSON'}, 400)
                    return
                url = req.get('url', '').strip()
                service = req.get('service', 'generic').strip()
                if not url:
                    self.send_json({'error': 'No URL provided'}, 400)
                    return
                payload = _format_payload(
                    service, 'ðŸ”” diskmind test',
                    'This is a test notification from diskmind.', 'info')
                send_url = url
                if service == 'ntfy':
                    parsed = urlparse(url)
                    topic = parsed.path.strip('/')
                    if topic:
                        payload['topic'] = topic
                        send_url = f"{parsed.scheme}://{parsed.netloc}/"
                success, error = _send_webhook(send_url, payload)
                if success:
                    self.send_json({'success': True})
                else:
                    self.send_json({'success': False, 'error': error or 'Webhook request failed'})
            
            elif path == '/api/collect':
                # Trigger data collection
                if self.fetch_script:
                    # Check for specific host in request body
                    try:
                        req_data = json.loads(body) if body else {}
                    except json.JSONDecodeError:
                        req_data = {}
                    
                    cmd = [sys.executable, self.fetch_script]
                    # If specific host requested, only fetch that one
                    if req_data.get('host'):
                        cmd.extend(['--hosts', req_data['host']])
                    
                    result = subprocess.run(
                        cmd,
                        capture_output=True,
                        text=True,
                        timeout=300
                    )
                    self.send_json({
                        'success': result.returncode == 0,
                        'output': result.stdout,
                        'error': result.stderr,
                    })
                else:
                    self.send_json({'error': 'Collect script not configured'}, 500)
            
            elif path == '/api/ingest':
                # Rate limit check
                client_ip = self.client_address[0]
                if not check_rate_limit(client_ip):
                    self.send_json({'error': 'Rate limited. Try again later.'}, 429)
                    return
                
                # Receive pushed SMART data from agents
                # Expected: CSV data in body, host in query param
                params = parse_qs(parsed.query)
                host = params.get('host', [None])[0]
                
                if not host:
                    self.send_json({'error': 'Missing host parameter'}, 400)
                    return
                
                if not body:
                    self.send_json({'error': 'Empty body'}, 400)
                    return
                
                # Validate push token if configured
                config = load_config()
                expected_token = config.get('push_token')
                if expected_token:
                    auth_header = self.headers.get('Authorization', '')
                    provided_token = ''
                    if auth_header.startswith('Bearer '):
                        provided_token = auth_header[7:]
                    if not hmac.compare_digest(provided_token, expected_token):
                        self.send_json({'error': 'Invalid or missing push token'}, 401)
                        return
                
                # Check host is configured for push
                raw_hosts = config.get('hosts', [])
                host_method = None
                for h in raw_hosts:
                    rest = h
                    if h.startswith('push:'): method, rest = 'push', h[5:]
                    elif h.startswith('ssh:'): method, rest = 'ssh', h[4:]
                    else: method = 'ssh'
                    ip = rest.split('@')[1] if '@' in rest else rest
                    if ip == host:
                        host_method = method
                        break
                
                if host_method is None:
                    # Log the push attempt from unknown host
                    try:
                        fm = get_fetch_module()
                        conn = fm.init_database(self.db_path)
                        conn.execute('''
                            INSERT INTO push_attempts (host, last_attempt, attempts, reason)
                            VALUES (?, ?, 1, 'unknown')
                            ON CONFLICT(host) DO UPDATE SET
                                last_attempt = excluded.last_attempt,
                                attempts = push_attempts.attempts + 1,
                                reason = 'unknown'
                        ''', (host, datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')))
                        conn.commit()
                        conn.close()
                    except Exception as e:
                        print(f"[warn] Failed to log push attempt for {host}: {e}", file=sys.stderr)
                    self.send_json({'error': f'Unknown host: {host}'}, 403)
                    return
                
                if host_method != 'push':
                    # Log the push attempt so frontend can show a hint
                    try:
                        fm = get_fetch_module()
                        conn = fm.init_database(self.db_path)
                        conn.execute('''
                            INSERT INTO push_attempts (host, last_attempt, attempts, reason)
                            VALUES (?, ?, 1, 'ssh')
                            ON CONFLICT(host) DO UPDATE SET
                                last_attempt = excluded.last_attempt,
                                attempts = push_attempts.attempts + 1,
                                reason = 'ssh'
                        ''', (host, datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')))
                        conn.commit()
                        conn.close()
                    except Exception as e:
                        print(f"[warn] Failed to log push attempt for {host}: {e}", file=sys.stderr)
                    self.send_json({'error': f'Host {host} is configured for SSH, not push'}, 403)
                    return
                
                try:
                    # Parse CSV and store using cached fetch module
                    fetch_module = get_fetch_module()
                    csv_data = body.decode('utf-8')
                    readings = fetch_module.parse_csv(csv_data, host=host)
                    
                    if not readings:
                        self.send_json({'error': 'No valid readings in CSV'}, 400)
                        return
                    
                    # Store in database
                    conn = fetch_module.init_database(self.db_path)
                    # Use client-provided scan timestamp if available (for buffered data)
                    scan_ts = self.headers.get('X-Scan-Timestamp')
                    if scan_ts:
                        try:
                            # Validate format
                            datetime.strptime(scan_ts.strip(), '%Y-%m-%d %H:%M:%S')
                            timestamp = scan_ts.strip()
                        except ValueError:
                            timestamp = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')
                    else:
                        timestamp = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')
                    fetch_module.store_readings(conn, readings, timestamp, source='push')
                    fetch_module.update_host_status(conn, host, 'ok', None, len(readings), timestamp)
                    # Clear any prior push attempts for this host
                    conn.execute('DELETE FROM push_attempts WHERE host = ?', (host,))
                    conn.commit()
                    
                    # Check for status changes and send notifications
                    thresholds = get_thresholds()
                    new_alerts = generate_alerts(conn, readings, thresholds, timestamp)
                    if new_alerts:
                        notify_config = config.get('notifications', {})
                        send_notifications(conn, new_alerts, notify_config,
                                           config.get('webhook_urls', []))
                    
                    conn.close()
                    
                    self.send_json({
                        'success': True,
                        'host': host,
                        'disks': len(readings),
                        'timestamp': timestamp,
                    })
                    
                except Exception as e:
                    self.send_json({'error': f'Failed to process data: {str(e)}'}, 500)
            
            elif path == '/api/alerts/acknowledge':
                try:
                    data = json.loads(body) if body else {}
                except json.JSONDecodeError:
                    self.send_json({'error': 'Invalid JSON'}, 400)
                    return
                
                alert_ids = data.get('ids', [])
                ack_all = data.get('all', False)
                
                conn = get_db_connection(self.db_path)
                if ack_all:
                    conn.execute('UPDATE alerts SET acknowledged = 1 WHERE acknowledged = 0')
                elif alert_ids:
                    placeholders = ','.join('?' * len(alert_ids))
                    conn.execute(f'UPDATE alerts SET acknowledged = 1 WHERE id IN ({placeholders})',
                                 alert_ids)
                conn.commit()
                conn.close()
                self.send_json({'success': True})
            
            elif path == '/api/push-approve':
                try:
                    data = json.loads(body) if body else {}
                except json.JSONDecodeError:
                    self.send_json({'error': 'Invalid JSON'}, 400)
                    return
                
                host = data.get('host')
                action = data.get('action')  # 'accept' or 'dismiss'
                
                if not host or action not in ('accept', 'dismiss'):
                    self.send_json({'error': 'Missing host or invalid action'}, 400)
                    return
                
                if action == 'accept':
                    # Add host as push:IP to config
                    config = load_config()
                    hosts = config.get('hosts', [])
                    entry = f'push:{host}'
                    if entry not in hosts:
                        hosts.append(entry)
                        save_config_list('hosts', hosts)
                
                # Clear push_attempts entry
                try:
                    fm = get_fetch_module()
                    conn = fm.init_database(self.db_path)
                    conn.execute('DELETE FROM push_attempts WHERE host = ?', (host,))
                    conn.commit()
                    conn.close()
                except Exception as e:
                    print(f"[warn] Failed to clear push attempts for {host}: {e}", file=sys.stderr)
                
                self.send_json({'success': True, 'action': action, 'host': host})
            
            else:
                self.send_response(404)
                self.end_headers()
        
        except Exception as e:
            self.send_json({'error': str(e)}, 500)


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(description='diskmind Web Server')
    parser.add_argument('-p', '--port', type=int, default=8080, help='Port (default: 8080)')
    parser.add_argument('--host', default='0.0.0.0', help='Host (default: 0.0.0.0)')
    parser.add_argument('--db', default='./data/diskmind.db', help='Database path')
    parser.add_argument('--dev', action='store_true', help='Dev mode: reload HTML template on every request')
    
    args = parser.parse_args()
    
    # Load config (uses parse_simple_yaml from diskmind_core)
    config = load_config()
    args.db = config.get('database', {}).get('path', args.db)
    
    # Initialize database (create if missing)
    if not os.path.exists(args.db):
        print(f"Database not found: {args.db} â€” creating empty database")
        fm = get_fetch_module()
        conn = fm.init_database(args.db)
        conn.close()
    
    # Set handler config
    SmartHTTPHandler.db_path = args.db
    SmartHTTPHandler.fetch_script = str(Path(__file__).parent / 'diskmind-fetch')
    
    # Dev mode: reload template on every request
    if args.dev:
        global _dev_mode
        _dev_mode = True
    
    # Start server
    server = ThreadingHTTPServer((args.host, args.port), SmartHTTPHandler)
    
    print(f"diskmind {VERSION}")
    print(f"=" * 40)
    print(f"Database: {args.db}")
    print(f"URL: http://{args.host}:{args.port}")
    if _dev_mode:
        print(f"Mode: dev (template reloads on every request)")
    print(f"")
    print(f"Press Ctrl+C to stop")
    print()
    
    try:
        server.serve_forever()
    except KeyboardInterrupt:
        print("\nShutting down...")
        server.shutdown()


if __name__ == '__main__':
    main()
